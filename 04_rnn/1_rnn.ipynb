{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f2789d735a9c9aa1457ec2fce57f3a3a",
     "grade": false,
     "grade_id": "cell-f5e46023398b0aab",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Deadline:</b> March 30, 2022 (Wednesday) 23:00\n",
    "</div>\n",
    "\n",
    "# Exercise 1. Sequence-to-sequence modeling with recurrent neural networks\n",
    "\n",
    "The goals of this exercise are\n",
    "* to get familiar with recurrent neural networks used for sequential data processing\n",
    "* to get familiar with the sequence-to-sequence model for machine translation\n",
    "* to learn PyTorch tools for batch processing of sequences with varying lengths\n",
    "* to learn how to write a custom `DataLoader`\n",
    "\n",
    "You may find it useful to look at this tutorial:\n",
    "* [Translation with a Sequence to Sequence Network and Attention](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_training = True  # Set this flag to True before validation and submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9cc4d569dc32e40fe066146a07b7c7b7",
     "grade": true,
     "grade_id": "evaluation_settings",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# During evaluation, this cell sets skip_training to True\n",
    "# skip_training = True\n",
    "\n",
    "import tools, warnings\n",
    "warnings.showwarning = tools.customwarn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import tools\n",
    "import tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data directory is /coursedata\n"
     ]
    }
   ],
   "source": [
    "# When running on your own computer, you can specify the data directory by:\n",
    "# data_dir = tools.select_data_dir('/your/local/data/directory')\n",
    "data_dir = tools.select_data_dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the device for training (use GPU if you have one)\n",
    "#device = torch.device('cuda:0')\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dbbca8fe9cf0cb1cb20dd200e23cfcb0",
     "grade": false,
     "grade_id": "cell-44cf6f3242607cde",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "if skip_training:\n",
    "    # The models are always evaluated on CPU\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "88cb529e1e7d2069f68d3df82a852cce",
     "grade": false,
     "grade_id": "cell-1f1e529682d7ce6d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Data\n",
    "\n",
    "The dataset that we are going to use consists of pairs of sentences in French and English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aa7408c0ea947cac397f5e35ce6498ee",
     "grade": false,
     "grade_id": "cell-6638bcc556bb02f2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from data import TranslationDataset, MAX_LENGTH, SOS_token, EOS_token\n",
    "\n",
    "trainset = TranslationDataset(data_dir, train=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ac3d437d5ade200a6ff8ca5407bf2c01",
     "grade": false,
     "grade_id": "cell-2d3c1dd7e239d2fa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "* `TranslationDataset` supports indexing as required by `torch.utils.data.Dataset`.\n",
    "* Sentences are tensors of maximum length `MAX_LENGTH`.\n",
    "* Words in a (sentence) tensor are represented as an index (integer) in a language vocabulary.\n",
    "* The string representation of a word from the source language can be obtained from index `i` with `dataset.input_lang.index2word[i]`.\n",
    "* Similarly for the target language `dataset.output_lang.index2word[j]`.\n",
    "\n",
    "Let us look at samples from that dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b55856502471f9f6a7f4a7f3699504b5",
     "grade": false,
     "grade_id": "cell-0d793aaf021670ff",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source sentence: \"nous en avons fini . EOS\"\n",
      "Sentence as tensor of word indices:\n",
      "tensor([123,  14, 124,  34,   5,   1])\n",
      "Target sentence: \"we re through . EOS\"\n",
      "Sentence as tensor of word indices:\n",
      "tensor([ 77,  78, 210,   4,   1])\n"
     ]
    }
   ],
   "source": [
    "src_sentence, tgt_sentence = trainset[np.random.choice(len(trainset))]\n",
    "print('Source sentence: \"%s\"' % ' '.join(trainset.input_lang.index2word[i.item()] for i in src_sentence))\n",
    "print('Sentence as tensor of word indices:')\n",
    "print(src_sentence)\n",
    "\n",
    "print('Target sentence: \"%s\"' % ' '.join(trainset.output_lang.index2word[i.item()] for i in tgt_sentence))\n",
    "print('Sentence as tensor of word indices:')\n",
    "print(tgt_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0d8116da735b2c5503a947814fd393b0",
     "grade": false,
     "grade_id": "cell-94d57799bcd1786b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of source-target pairs in the training set:  8682\n"
     ]
    }
   ],
   "source": [
    "print('Number of source-target pairs in the training set: ', len(trainset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8c5eec5bf6f7b88396659a1cdc5faa2c",
     "grade": false,
     "grade_id": "cell-80e91375dcb62ed2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Sequence-to-sequence model for machine translation\n",
    "\n",
    "In this exercise, we are going to build a machine translation system which transforms a sentence in one language into a sentence in another one. The computational graph of the translation model is shown below:\n",
    "\n",
    "<img src=\"seq2seq.png\" width=900>\n",
    "\n",
    "We are going to use a simplified model without the dotted connections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f00d388d9b131f954d70a363b071d0f6",
     "grade": false,
     "grade_id": "cell-86482ed71ea81ed3",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Custom DataLoader\n",
    "\n",
    "We would like to train the sequence-to-sequence model using mini-batch training.\n",
    "One difficulty of mini-batch training in this case is that sequences may have varying lengths and this has to be taken into account when building the computational graph. Luckily, PyTorch has tools to support batch processing of such sequences.\n",
    "To use those tools, we need to write a custom data loader which puts sequences of varying lengths in the same tensor. We can customize the data loader by providing a custom `collate_fn` as explained [here](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader).\n",
    "\n",
    "Our collate function:\n",
    "- combines sequences from the source language in a single tensor with extra values (at the end) filled with `PADDING_VALUE=0`.\n",
    "- combines sequences from the target language in a single tensor with extra values (at the end) filled with `PADDING_VALUE=0`.\n",
    "\n",
    "**Important**:\n",
    "- Later in the code (not in this `collate` function), we will convert source sequences to objects of class [`PackedSequence`](https://pytorch.org/docs/stable/nn.html?highlight=packedsequence#torch.nn.utils.rnn.PackedSequence) which can be processed by recurrent units such as `GRU` or `LSTM`. Conversion to `PackedSequence` objects requires sequences to be sorted by their lengths.\n",
    "**Therefore, the returned source sequences should be sorted by length in a decreasing order.**\n",
    "* The target sequences need not be sorted by their lengths because we have to keep the same order of sequences in the source and target tensors.\n",
    "\n",
    "Your task is to implement the collate function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "523f0d6ef1a7a3716d4f0e47fe2ada46",
     "grade": false,
     "grade_id": "cell-f7cf358c436c6d49",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "PADDING_VALUE = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d791959e8c9e2b324dec5a664ab99b26",
     "grade": false,
     "grade_id": "collate",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate(list_of_samples):\n",
    "    \"\"\"Merges a list of samples to form a mini-batch.\n",
    "\n",
    "    Args:\n",
    "      list_of_samples is a list of tuples (src_seq, tgt_seq):\n",
    "          src_seq is of shape (src_seq_length,)\n",
    "          tgt_seq is of shape (tgt_seq_length,)\n",
    "\n",
    "    Returns:\n",
    "      src_seqs of shape (max_src_seq_length, batch_size): Tensor of padded source sequences.\n",
    "          The sequences should be sorted by length in a decreasing order, that is src_seqs[:,0] should be\n",
    "          the longest sequence, and src_seqs[:,-1] should be the shortest.\n",
    "      src_seq_lengths: List of lengths of source sequences.\n",
    "      tgt_seqs of shape (max_tgt_seq_length, batch_size): Tensor of padded target sequences.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    src_seqs = [entry[0] for entry in list_of_samples]\n",
    "    tgt_seqs = [entry[1] for entry in list_of_samples]\n",
    "    \n",
    "    src_lengths = [entry.shape[0] for entry in src_seqs]\n",
    "    tgt_lengths = [entry.shape[0] for entry in tgt_seqs]\n",
    "    \n",
    "    sorted_src_len_ind = np.argsort(src_lengths)\n",
    "    \n",
    "    src_sorted = []\n",
    "    src_seq_lengths = []\n",
    "    tgt_sorted = []\n",
    "    for i, ind in enumerate(sorted_src_len_ind[::-1]):\n",
    "        src_seq_lengths.append(src_lengths[ind])\n",
    "        \n",
    "        temp1 = torch.zeros([max(src_lengths)])\n",
    "        temp2 = torch.zeros([max(tgt_lengths)])\n",
    "        \n",
    "        temp1[:len(src_seqs[ind])] = src_seqs[ind]\n",
    "        temp2[:len(tgt_seqs[ind])] = tgt_seqs[ind]\n",
    "        src_sorted.append(temp1)\n",
    "        tgt_sorted.append(temp2)\n",
    "    \n",
    "#     print(src_sorted)\n",
    "#     print(tgt_sorted)\n",
    "    \n",
    "    src_sorted = torch.stack(src_sorted).T\n",
    "    tgt_sorted = torch.stack(tgt_sorted).T\n",
    "    \n",
    "#     print(src_sorted, src_sorted.shape)\n",
    "    \n",
    "#     src_seqs = torch.zeros([max_len, batch_size])\n",
    "            \n",
    "    return src_sorted.long(), src_seq_lengths, tgt_sorted.long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e4556bc267765daec3cd760ab352b33a",
     "grade": false,
     "grade_id": "cell-946d5fa69247a122",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    }
   ],
   "source": [
    "def test_collate_shapes():\n",
    "    pairs = [\n",
    "        (torch.LongTensor([1, 2]), torch.LongTensor([3, 4, 5])),\n",
    "        (torch.LongTensor([6, 7, 8]), torch.LongTensor([9, 10])),\n",
    "    ]\n",
    "    pad_src_seqs, src_seq_lengths, pad_tgt_seqs = collate(pairs)\n",
    "    assert type(src_seq_lengths) == list, \"src_seq_lengths should be a list.\"\n",
    "    assert pad_src_seqs.shape == torch.Size([3, 2]), f\"Bad pad_src_seqs.shape: {pad_src_seqs.shape}\"\n",
    "    assert pad_src_seqs.dtype == torch.long\n",
    "    assert pad_tgt_seqs.shape == torch.Size([3, 2]), f\"Bad pad_tgt_seqs.shape: {pad_tgt_seqs.shape}\"\n",
    "    assert pad_tgt_seqs.dtype == torch.long\n",
    "    print('Success')\n",
    "\n",
    "test_collate_shapes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c7f371320f69fbb2a05acb069a558a74",
     "grade": true,
     "grade_id": "test_collate_fn",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell tests collate() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4dc67bc5d1c00931a6cb4a0fdf840b71",
     "grade": false,
     "grade_id": "cell-b4fb631ef92b42cd",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# We create custom DataLoader using the implemented collate function\n",
    "# We are going to process 64 sequences at the same time (batch_size=64)\n",
    "trainloader = DataLoader(dataset=trainset, batch_size=64, shuffle=True, collate_fn=collate, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ea1cc5c8b8d4736b25686e444c996848",
     "grade": false,
     "grade_id": "cell-3f6dfc8dc7015270",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Encoder\n",
    "\n",
    "The encoder encodes a source sequence $(x_1, x_2, ..., x_T)$ into a single vector $h_T$ using the following recursion:\n",
    "$$\n",
    "  h_{t} = f(h_{t-1}, x_t) \\qquad t = 1, \\ldots, T\n",
    "$$\n",
    "where:\n",
    "* intial state $h_0$ is often chosen arbitrarily (we choose it to be zero)\n",
    "* function $f$ is defined by the type of the RNN cell (in our experiments, we will use [GRU](https://pytorch.org/docs/stable/nn.html#torch.nn.GRU))\n",
    "* $x_t$ is a vector that represents the $t$-th word in the source sentence.\n",
    "\n",
    "A common practice in natural language processing is to _learn_ the word representations $x_t$ (instead of, for example, using one-hot coded vectors). In PyTorch, this is supported by class [Embedding](https://pytorch.org/docs/stable/nn.html#torch.nn.Embedding) which we are going to use.\n",
    "\n",
    "The computational graph of the encoder is shown below:\n",
    "\n",
    "<img src=\"seq2seq_encoder.png\" width=500>\n",
    "\n",
    "Your task is to implement the `forward` function of the encoder. It should contain the following steps:\n",
    "* Embed the words of the source sequences.\n",
    "* Pack source sequences using [`pack_padded_sequence`](https://pytorch.org/docs/stable/nn.html?highlight=pack_padded_sequence#torch.nn.utils.rnn.pack_padded_sequence). This converts padded source sequences into an object that can be processed by PyTorch recurrent units such as `nn.GRU` or `nn.LSTM`.\n",
    "* Apply GRU computations to packed sequences obtained in the previous step\n",
    "* Convert packed sequence of GRU outputs into padded representation with [`pad_packed_sequence`](https://pytorch.org/docs/stable/nn.html?highlight=pad_packed_sequence#torch.nn.utils.rnn.pad_packed_sequence)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8ad27026a59c87defe4742f2afbcabd4",
     "grade": false,
     "grade_id": "Encoder",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, src_dictionary_size, embed_size, hidden_size):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          src_dictionary_size: The number of words in the source dictionary.\n",
    "          embed_size: The number of dimensions in the word embeddings.\n",
    "          hidden_size: The number of features in the hidden state of GRU.\n",
    "        \"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(src_dictionary_size, embed_size)\n",
    "        self.gru = nn.GRU(input_size=embed_size, hidden_size=hidden_size)\n",
    "\n",
    "    def forward(self, pad_seqs, seq_lengths, hidden):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          pad_seqs of shape (max_seq_length, batch_size): Padded source sequences.\n",
    "          seq_lengths: List of sequence lengths.\n",
    "          hidden of shape (1, batch_size, hidden_size): Initial states of the GRU.\n",
    "\n",
    "        Returns:\n",
    "          outputs of shape (max_seq_length, batch_size, hidden_size): Padded outputs of GRU at every step.\n",
    "          hidden of shape (1, batch_size, hidden_size): Updated states of the GRU.\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        embedded = self.embedding(pad_seqs)\n",
    "        embedded = nn.utils.rnn.pack_padded_sequence(embedded, seq_lengths, batch_first=False)\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "    \n",
    "        output, _ = nn.utils.rnn.pad_packed_sequence(output, batch_first=False)\n",
    "    \n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size=1):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9b26ce62da6c7f9b9f9370cac4ebe9d6",
     "grade": false,
     "grade_id": "cell-33422e03c9970c2a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    }
   ],
   "source": [
    "def test_Encoder_shapes():\n",
    "    hidden_size = 3\n",
    "    encoder = Encoder(src_dictionary_size=5, embed_size=10, hidden_size=hidden_size)\n",
    "\n",
    "    max_seq_length = 4\n",
    "    batch_size = 2\n",
    "    hidden = encoder.init_hidden(batch_size=batch_size)\n",
    "    pad_seqs = torch.tensor([\n",
    "        [        1,             2],\n",
    "        [        2,     EOS_token],\n",
    "        [        3, PADDING_VALUE],\n",
    "        [EOS_token, PADDING_VALUE]\n",
    "    ])\n",
    "\n",
    "    outputs, new_hidden = encoder.forward(pad_seqs=pad_seqs, seq_lengths=[4, 2], hidden=hidden)\n",
    "    assert outputs.shape == torch.Size([4, batch_size, hidden_size]), f\"Bad outputs.shape: {outputs.shape}\"\n",
    "    assert new_hidden.shape == torch.Size([1, batch_size, hidden_size]), f\"Bad new_hidden.shape: {new_hidden.shape}\"\n",
    "    print('Success')\n",
    "\n",
    "test_Encoder_shapes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c612bae3dc4da49e7e1a885df23acac7",
     "grade": true,
     "grade_id": "test_Encoder",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell tests Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "078cb4df64e3607355197dff52f07d9a",
     "grade": false,
     "grade_id": "cell-3133e50590987e56",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Decoder\n",
    "\n",
    "The decoder takes as input the representation computed by the encoder and transforms it into a sentence in the target language. The computational graph of the decoder is shown below:\n",
    "\n",
    "<img src=\"seq2seq_decoder.png\" width=500 align=\"top\">\n",
    "\n",
    "* $z_0$ is the output of the encoder, that is $z_0 = h_5$, thus `hidden_size` of the decoder should be the same as `hidden_size` of the encoder.\n",
    "* $y_{i}$ are the log-probabilities of the words in the target language, the dimensionality of $y_{i}$ is the size of the target dictionary.\n",
    "* $z_{i}$ is mapped to $y_{i}$ using a linear layer `self.out` followed by `F.log_softmax` (because we use `nn.NLLLoss` loss for training).\n",
    "* Each cell of the decoder is a GRU, it receives as inputs the previous state $z_{i-1}$ and relu of the **embedding** of the previous word. Thus, you need to embed the words of the target language as well. The previous word is taken as the word with the maximum log-probability.\n",
    "\n",
    "Note that the decoder outputs a word at every step and the same word is used as the input to the recurrent unit at the next step. At the beginning of decoding, the previous word input is fed with a special word SOS which stands for \"start of a sentence\". During training, we know the target sentence for decoding, therefore we can feed the correct words $y_i$ as inputs to the recurrent unit.\n",
    "\n",
    "There is one extra thing that it is wise to take care of. When the target sentence is fed to the decoder during training, the decoder learns to generate only the next word (this scenario is called \"teacher forcing\"). At test time, the decoder works differently: it generates the whole sequence using its own predictions as inputs at each step. Therefore, it makes sense to train the decoder to produce full sentences. In order to do that, we will alternate between two modes during training:\n",
    "* \"teacher forcing\": the decoder is fed with the words in the target sequence\n",
    "* no \"teacher forcing\": the decoder generates the output sequence using its own predictions. In this case, we will generate sequences of the same length as the length of the longest sequence in `pad_tgt_seqs` (if `pad_tgt_seqs` is not `None`) or of length `MAX_LENGTH` (if `pad_tgt_seqs` is `None`).\n",
    "\n",
    "You need to implement the decoder which has the structure shown in the figure above.\n",
    "\n",
    "Notes:\n",
    "* `SOS_token` is imported at the beginning of the notebook.\n",
    "* **Running this code on GPU sometimes fails producing a CUDA error (if you know the reason, please let us know).** If this happens to you, please train the model on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "70b2c4f47ee96519de22564de0a2f192",
     "grade": false,
     "grade_id": "Decoder",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, tgt_dictionary_size, embed_size, hidden_size):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          tgt_dictionary_size: The number of words in the target dictionary.\n",
    "          embed_size: The number of dimensions in the word embeddings.\n",
    "          hidden_size: The number of features in the hidden state.\n",
    "        \"\"\"\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(tgt_dictionary_size, embed_size)\n",
    "        self.gru = nn.GRU(input_size=embed_size, hidden_size=hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, tgt_dictionary_size)\n",
    "\n",
    "    def forward(self, hidden, pad_tgt_seqs=None, teacher_forcing=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          hidden of shape (1, batch_size, hidden_size): States of the GRU.\n",
    "          pad_tgt_seqs of shape (max_out_seq_length, batch_size): Tensor of words (word indices) of the\n",
    "              target sentence. If None, the output sequence is generated by feeding the decoder's outputs\n",
    "              (teacher_forcing has to be False).\n",
    "          teacher_forcing (bool): Whether to use teacher forcing or not.\n",
    "\n",
    "        Returns:\n",
    "          outputs of shape (max_out_seq_length, batch_size, tgt_dictionary_size): Tensor of log-probabilities\n",
    "              of words in the target language.\n",
    "          hidden of shape (1, batch_size, hidden_size): New states of the GRU.\n",
    "\n",
    "        Note: Do not forget to transfer tensors that you may want to create in this function to the device\n",
    "        specified by `hidden.device`.\n",
    "        \"\"\"\n",
    "        if pad_tgt_seqs is None:\n",
    "            assert not teacher_forcing, 'Cannot use teacher forcing without a target sequence.'\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        decoder_input = torch.tensor([[SOS_token]*hidden.shape[1]], device=hidden.device)\n",
    "#         print(decoder_input.shape)\n",
    "        \n",
    "        if not teacher_forcing:\n",
    "#             print('without forcing')\n",
    "            decoder_outputs = []\n",
    "            max_len = MAX_LENGTH if pad_tgt_seqs is None else pad_tgt_seqs.shape[0]\n",
    "            for i in range(max_len):\n",
    "                embedded = self.embedding(decoder_input)\n",
    "#                 print(embedded.shape)\n",
    "                embedded = F.relu(embedded)\n",
    "                output, hidden = self.gru(embedded, hidden)\n",
    "#                 output = F.log_softmax(self.out(output.squeeze()), dim=1)\n",
    "#                 print(output.shape)\n",
    "                output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "                topv, topi = output.topk(1)\n",
    "                decoder_outputs.append(output)\n",
    "                decoder_input = topi.detach().transpose(0,1)          \n",
    "            decoder_outputs = torch.stack(decoder_outputs)\n",
    "            return decoder_outputs, hidden\n",
    "        \n",
    "        else:\n",
    "#             print('with forcing')\n",
    "            decoder_outputs = []\n",
    "            max_len = MAX_LENGTH if pad_tgt_seqs is None else pad_tgt_seqs.shape[0]\n",
    "            for i in range(max_len):\n",
    "                embedded = self.embedding(decoder_input)\n",
    "#                 print(embedded.shape)\n",
    "                embedded = F.relu(embedded)\n",
    "                output, hidden = self.gru(embedded, hidden)\n",
    "                output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "                topv, topi = output.topk(1)\n",
    "                decoder_outputs.append(output)\n",
    "                decoder_input = pad_tgt_seqs[i].unsqueeze(0)\n",
    "#                 print(decoder_input.shape)\n",
    "            decoder_outputs = torch.stack(decoder_outputs)\n",
    "            return decoder_outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bb5dd456e3e629a59f81291bfb10586a",
     "grade": false,
     "grade_id": "cell-f0749cc463edb855",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    }
   ],
   "source": [
    "def test_Decoder_shapes():\n",
    "    hidden_size = 2\n",
    "    tgt_dictionary_size = 5\n",
    "    test_decoder = Decoder(tgt_dictionary_size, embed_size=10, hidden_size=hidden_size)\n",
    "\n",
    "    max_seq_length = 4\n",
    "    batch_size = 2\n",
    "    pad_tgt_seqs = torch.tensor([\n",
    "        [        1,             2],\n",
    "        [        2,     EOS_token],\n",
    "        [        3, PADDING_VALUE],\n",
    "        [EOS_token, PADDING_VALUE]\n",
    "    ])  # [max_seq_length, batch_size]\n",
    "\n",
    "    hidden = torch.zeros(1, batch_size, hidden_size)\n",
    "    outputs, new_hidden = test_decoder.forward(hidden, pad_tgt_seqs, teacher_forcing=False)\n",
    "\n",
    "    assert outputs.size(0) <= 4, f\"Too long output sequence: outputs.size(0)={outputs.size(0)}\"\n",
    "    assert outputs.shape[1:] == torch.Size([batch_size, tgt_dictionary_size]), \\\n",
    "        f\"Bad outputs.shape[1:]={outputs.shape[1:]}\"\n",
    "    assert new_hidden.shape == torch.Size([1, batch_size, hidden_size]), f\"Bad new_hidden.shape={new_hidden.shape}\"\n",
    "\n",
    "    outputs, new_hidden = test_decoder.forward(hidden, pad_tgt_seqs, teacher_forcing=True)\n",
    "    assert outputs.shape == torch.Size([4, batch_size, tgt_dictionary_size]), \\\n",
    "        f\"Bad shape outputs.shape={outputs.shape}\"\n",
    "    assert new_hidden.shape == torch.Size([1, batch_size, hidden_size]), f\"Bad new_hidden.shape={new_hidden.shape}\"\n",
    "\n",
    "    # Generation mode\n",
    "    outputs, new_hidden = test_decoder.forward(hidden, None, teacher_forcing=False)\n",
    "    assert outputs.shape[1:] == torch.Size([batch_size, tgt_dictionary_size]), \\\n",
    "        f\"Bad outputs.shape[1:]={outputs.shape[1:]}\"\n",
    "    assert new_hidden.shape == torch.Size([1, batch_size, hidden_size]), f\"Bad new_hidden.shape={new_hidden.shape}\"\n",
    "\n",
    "    print('Success')\n",
    "\n",
    "test_Decoder_shapes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "06db3e79dd4062d9884c92b2d7d5446d",
     "grade": true,
     "grade_id": "test_Decoder",
     "locked": true,
     "points": 0.8,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell tests Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ccb2c0a30b0b979ac8c69d76301b5370",
     "grade": true,
     "grade_id": "cell-056d8c00b620337b",
     "locked": true,
     "points": 0.6,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell tests Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c457795f7e0ed5a5f1c62c8aba814ed2",
     "grade": true,
     "grade_id": "cell-c5cc115725c6508b",
     "locked": true,
     "points": 0.6,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell tests Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "876b987bce981f6692a1847ba12a888f",
     "grade": false,
     "grade_id": "cell-6207d3c96c443b4f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Training of sequence-to-sequence model using mini-batches\n",
    "\n",
    "Now we are going to train the sequence-to-sequence model on the toy translation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a3a4c7142fd6d3aea8ec9bd599f32b68",
     "grade": false,
     "grade_id": "cell-dc6ed9b2b10473c5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Create the seq2seq model\n",
    "hidden_size = embed_size = 256\n",
    "encoder = Encoder(trainset.input_lang.n_words, embed_size, hidden_size).to(device)\n",
    "decoder = Decoder(trainset.output_lang.n_words, embed_size, hidden_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "529e79ae57187958b239781c17b99635",
     "grade": false,
     "grade_id": "cell-6ce059e5ee375d3b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "31e5cb3f3cda9dac28a5d1562df92e24",
     "grade": false,
     "grade_id": "cell-b1645c0797a9d5f6",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Implement the training loop in the cell below. In the training loop, we first encode source sequences using the encoder, then we decode the encoded state using the decoder. The decoder outputs log-probabilities of words in the target language. We need to use these log-probabilities and the indexes of the words in the target sequences to compute the loss.\n",
    "\n",
    "The loss is\n",
    "\\begin{align*}\n",
    "L = - \\frac{1}{N} \\sum_{n} \\sum_{t=1}^{T_n}\n",
    "\\log p\\left(\\mathbf{y}_t^{(n)} \\:\\Bigl|\\: \\mathbf{y}_{<t}^{(n)}, \\mathbf{X}^{(n)} \\right)\n",
    "\\end{align*}\n",
    "where $T_n$ is the length of the $n$-th target sequence and $N= \\sum_{n=1} T_n$ is the total number of words in all the sentences of the mini-batch.\n",
    "\n",
    "Recommended hyperparameters:\n",
    "- Encoder optimizer: Adam with learning rate 0.001\n",
    "- Decoder optimizer: Adam with learning rate 0.001\n",
    "- Number of epochs: 30\n",
    "- Toggle `teacher_forcing` on and off (for each mini-batch) according to the `teacher_forcing_ratio` specified above.\n",
    "\n",
    "Hints:\n",
    "- Training should proceed relatively fast.\n",
    "- If you do well, the training loss should reach 0.1 in 30 epochs.\n",
    "- Slight overlearning may happen (you can see that if you track the test error during training) but you can ignore this problem. \n",
    "- **Important:** When computing the loss, you need to ignore the padded values. This can easily be done by using argument `ignore_index` of function [`nll_loss`](\n",
    "https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.nll_loss)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e7a530b833c54e4e7de2bac7b3b235c0",
     "grade": false,
     "grade_id": "cell-39d518fd8074b9ee",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, train_loss: 3.7521231560146107\n",
      "epoch: 1, train_loss: 2.8303913596798393\n",
      "epoch: 2, train_loss: 2.4709043239845947\n",
      "epoch: 3, train_loss: 2.1992516535169937\n",
      "epoch: 4, train_loss: 1.993794463136617\n",
      "epoch: 5, train_loss: 1.770504866452778\n",
      "epoch: 6, train_loss: 1.582564906162374\n",
      "epoch: 7, train_loss: 1.396722435074694\n",
      "epoch: 8, train_loss: 1.1995192380512463\n",
      "epoch: 9, train_loss: 1.054234921493951\n",
      "epoch: 10, train_loss: 0.9172429867527064\n",
      "epoch: 11, train_loss: 0.7939215880983016\n",
      "epoch: 12, train_loss: 0.6668852291562978\n",
      "epoch: 13, train_loss: 0.5594071789699442\n",
      "epoch: 14, train_loss: 0.47996531536473946\n",
      "epoch: 15, train_loss: 0.38093397930702744\n",
      "epoch: 16, train_loss: 0.3344926998457488\n",
      "epoch: 17, train_loss: 0.27510302272789616\n",
      "epoch: 18, train_loss: 0.23044458886279778\n",
      "epoch: 19, train_loss: 0.19316143520614681\n",
      "epoch: 20, train_loss: 0.16667605761219473\n",
      "epoch: 21, train_loss: 0.1407621519859223\n",
      "epoch: 22, train_loss: 0.12151053761515547\n",
      "epoch: 23, train_loss: 0.10548684020143222\n",
      "epoch: 24, train_loss: 0.09328865993987112\n",
      "epoch: 25, train_loss: 0.08584462766371229\n",
      "epoch: 26, train_loss: 0.07914563347859417\n",
      "epoch: 27, train_loss: 0.07629051662104971\n",
      "epoch: 28, train_loss: 0.06897214268717695\n",
      "epoch: 29, train_loss: 0.06613877147217007\n"
     ]
    }
   ],
   "source": [
    "if not skip_training:\n",
    "    # YOUR CODE HERE\n",
    "    criterion = nn.NLLLoss(ignore_index=PADDING_VALUE, reduction='mean')\n",
    "    \n",
    "    enc_optimizer = optim.Adam(encoder.parameters(), lr=1e-3)\n",
    "    dec_optimizer = optim.Adam(decoder.parameters(), lr=1e-3)\n",
    "    for e in range(30):\n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "        \n",
    "        train_loss = 0\n",
    "    \n",
    "        for batch_idx, (pad_src_seqs, src_seq_lengths, pad_tgt_seqs) in enumerate(trainloader):\n",
    "            \n",
    "            #print('batch: ', batch_idx, )\n",
    "            \n",
    "            pad_src_seqs, pad_tgt_seqs = pad_src_seqs.to(device), pad_tgt_seqs.to(device)\n",
    "            \n",
    "            enc_optimizer.zero_grad()\n",
    "            dec_optimizer.zero_grad()\n",
    "            \n",
    "            hidden = encoder.init_hidden(batch_size=len(src_seq_lengths)).to(device)\n",
    "\n",
    "            _, new_hidden = encoder.forward(pad_seqs=pad_src_seqs, seq_lengths=src_seq_lengths, hidden=hidden)\n",
    "            is_teacher = random.random() <= teacher_forcing_ratio\n",
    "            \n",
    "            outputs, new_hidden = decoder.forward(new_hidden, pad_tgt_seqs, teacher_forcing=is_teacher)\n",
    "            \n",
    "            loss = criterion(outputs.permute(1, 2, 0), pad_tgt_seqs.permute(1,0))\n",
    "            loss.backward()\n",
    "            enc_optimizer.step()\n",
    "            dec_optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            \n",
    "        train_loss /= len(trainloader)\n",
    "        print('epoch: {}, train_loss: {}'.format(e, train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do you want to save the model (type yes to confirm)? yes\n",
      "Model saved to 1_rnn_encoder.pth.\n",
      "Do you want to save the model (type yes to confirm)? yes\n",
      "Model saved to 1_rnn_decoder.pth.\n"
     ]
    }
   ],
   "source": [
    "# Save the model to disk (the pth-files will be submitted automatically together with your notebook)\n",
    "# Set confirm=False if you do not want to be asked for confirmation before saving.\n",
    "if not skip_training:\n",
    "    tools.save_model(encoder, '1_rnn_encoder.pth', confirm=True)\n",
    "    tools.save_model(decoder, '1_rnn_decoder.pth', confirm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "41f02c333feda1857a71e152a65a1d53",
     "grade": false,
     "grade_id": "cell-cb9033683841ebab",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "if skip_training:\n",
    "    hidden_size = 256\n",
    "    encoder = Encoder(trainset.input_lang.n_words, embed_size, hidden_size)\n",
    "    tools.load_model(encoder, '1_rnn_encoder.pth', device)\n",
    "    \n",
    "    decoder = Decoder(trainset.output_lang.n_words, embed_size, hidden_size)\n",
    "    tools.load_model(decoder, '1_rnn_decoder.pth', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6f3ba6e63d83529028b6a4aab2ee7d9f",
     "grade": true,
     "grade_id": "test_accuracy",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell tests training accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "96a617d060e534a0f6981e6e7c20d00b",
     "grade": false,
     "grade_id": "cell-25e4072e5588afaa",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Evaluation\n",
    "\n",
    "Next we need to implement a function that converts input sequences to output sequences using the trained sequence-to-sequence model.\n",
    "\n",
    "Notes:\n",
    "* Since we do not need to compute the gradients in the evaluation phase, we can speed up the computations by using the statement `with torch.no_grad():`.\n",
    "* Please transfer the tensors to `device` inside this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1a3ce2d2714284bbe39dc0dd879f3478",
     "grade": false,
     "grade_id": "cell-1d90b47e7cbf1cc9",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def translate(encoder, decoder, pad_src_seqs, src_seq_lengths):\n",
    "    \"\"\"Translate sequences from the source language to the target language using the trained model.\n",
    "    \n",
    "    Args:\n",
    "      encoder (Encoder): Trained encoder.\n",
    "      decoder (Decoder): Trained decoder.\n",
    "      pad_src_seqs of shape (max_src_seq_length, batch_size): Padded source sequences.\n",
    "      src_seq_lengths: List of source sequence lengths.\n",
    "    \n",
    "    Returns:\n",
    "      out_seqs of shape (MAX_LENGTH, batch_size): LongTensor of word indices of the output sequences.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pad_src_seqs = pad_src_seqs.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        hidden = encoder.init_hidden(batch_size=len(src_seq_lengths)).to(device)\n",
    "        \n",
    "        outputs , new_hidden = encoder.forward(pad_seqs=pad_src_seqs, seq_lengths=src_seq_lengths, hidden=hidden)\n",
    "    \n",
    "        outputs, new_hidden = decoder.forward(new_hidden, None, teacher_forcing=False)\n",
    "        \n",
    "        _, outputs = outputs.topk(1)\n",
    "        outputs = outputs.squeeze(2)\n",
    "\n",
    " #       print(outputs.shape)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c459aea453b88e64d5be88edbe651f95",
     "grade": false,
     "grade_id": "cell-149ce7a6eeca1021",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    }
   ],
   "source": [
    "def test_translate_shapes():\n",
    "    pad_src_seqs = torch.tensor([\n",
    "        [1, 2],\n",
    "        [2, 3],\n",
    "        [3, 0],\n",
    "        [4, 0]\n",
    "    ])\n",
    "\n",
    "    out_seqs = translate(encoder, decoder, pad_src_seqs, src_seq_lengths=[4, 2])\n",
    "    assert out_seqs.shape == torch.Size([MAX_LENGTH, 2]), f\"Wrong out_seqs.shape: {out_seqs.shape}\"\n",
    "    print('Success')\n",
    "\n",
    "test_translate_shapes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a509243e324c5111aaf064a0aaf64b56",
     "grade": false,
     "grade_id": "cell-caa93c3b88e0d8e0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let us now translate a few sentences from the training set and print the source, target, and produced output.\n",
    "\n",
    "If you trained the model well enough, the model should memorize the training data well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "98212370873fb2a1bc0d0189cbc6ea36",
     "grade": false,
     "grade_id": "cell-444ba3ba61c6db91",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def seq_to_tokens(seq, lang):\n",
    "    'Convert a sequence of word indices into a list of words (strings).'\n",
    "    sentence = []\n",
    "    for i in seq:\n",
    "        if i == EOS_token:\n",
    "            break\n",
    "        sentence.append(lang.index2word[i.item()])\n",
    "    return(sentence)\n",
    "\n",
    "def seq_to_string(seq, lang):\n",
    "    'Convert a sequence of word indices into a sentence string.'\n",
    "    return(' '.join(seq_to_tokens(seq, lang)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "82f902fa3e4af596b501085007b0c419",
     "grade": false,
     "grade_id": "cell-e3bfe231f4ca66d2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate training data:\n",
      "-----------------------------\n",
      "SRC: ses meilleures annees sont derriere lui tu sais .\n",
      "TGT: he is over the hill you know .\n",
      "OUT: he is over the hill you know .\n",
      "\n",
      "SRC: tu es en train d effrayer les enfants .\n",
      "TGT: you re scaring the kids .\n",
      "OUT: you re scaring the kids .\n",
      "\n",
      "SRC: elle a une conception cynique de la vie .\n",
      "TGT: she is very cynical about life .\n",
      "OUT: she is very cynical about life .\n",
      "\n",
      "SRC: il est a l article de la mort .\n",
      "TGT: he is about to die .\n",
      "OUT: he is about to die .\n",
      "\n",
      "SRC: je ne vais pas jouer a ce jeu .\n",
      "TGT: i m not going to play this game .\n",
      "OUT: i m not going to play this game .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Translate a few sentences from the training set\n",
    "print('Translate training data:')\n",
    "print('-----------------------------')\n",
    "pad_src_seqs, src_seq_lengths, pad_tgt_seqs = next(iter(trainloader))\n",
    "out_seqs = translate(encoder, decoder, pad_src_seqs, src_seq_lengths)\n",
    "\n",
    "for i in range(5):\n",
    "    print('SRC:', seq_to_string(pad_src_seqs[:,i], trainset.input_lang))\n",
    "    print('TGT:', seq_to_string(pad_tgt_seqs[:,i], trainset.output_lang))\n",
    "    print('OUT:', seq_to_string(out_seqs[:,i], trainset.output_lang))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "249b5575795aa35f76ff5c9122471fe9",
     "grade": false,
     "grade_id": "cell-66cbefeb6952d610",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now we translate random sentences from the test set. A well-trained model should output sentences that look similar to the target ones. The mistakes are usually done for words that were rare in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bc007c19783ab3b0e6f1a8ca7bf2bc5b",
     "grade": false,
     "grade_id": "cell-cd6dbabc07e7e01d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "testset = TranslationDataset(data_dir, train=False)\n",
    "testloader = DataLoader(dataset=testset, batch_size=64, shuffle=False, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "01a04d9631b3e26c2df1dc8d59688a17",
     "grade": false,
     "grade_id": "cell-135c99847babe153",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate test data:\n",
      "-----------------------------\n",
      "SRC: je suis en train de boire une biere .\n",
      "TGT: i m drinking a beer .\n",
      "OUT: i m drinking a magazine .\n",
      "\n",
      "SRC: elle est entierement devouee a ses trois enfants .\n",
      "TGT: she is devoted to her three children .\n",
      "OUT: she is devoted to her children .\n",
      "\n",
      "SRC: il est toujours en train de se plaindre .\n",
      "TGT: he is constantly complaining .\n",
      "OUT: he is always complaining .\n",
      "\n",
      "SRC: c est un expert du lancer de couteaux .\n",
      "TGT: he s an expert at throwing knives .\n",
      "OUT: he is a man of his word .\n",
      "\n",
      "SRC: tu n es pas aussi maligne que moi .\n",
      "TGT: you re not as smart as me .\n",
      "OUT: you re not as smart as me .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Translate test data:')\n",
    "print('-----------------------------')\n",
    "pad_src_seqs, src_seq_lengths, pad_tgt_seqs = next(iter(testloader))\n",
    "out_seqs = translate(encoder, decoder, pad_src_seqs, src_seq_lengths)\n",
    "\n",
    "for i in range(5):\n",
    "    print('SRC:', seq_to_string(pad_src_seqs[:,i], testset.input_lang))\n",
    "    print('TGT:', seq_to_string(pad_tgt_seqs[:,i], testset.output_lang))\n",
    "    print('OUT:', seq_to_string(out_seqs[:,i], testset.output_lang))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e08dc0b10bca100c75eaa85789c9ea27",
     "grade": false,
     "grade_id": "cell-1cfd09f0808e511c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Compute BLEU score\n",
    "\n",
    "Let us now compute the [BLEU score](https://en.wikipedia.org/wiki/BLEU) for the translations produced by our model. We can use the PyTorch function [bleu_score](https://pytorch.org/text/data_metrics.html#torchtext.data.metrics.bleu_score) for that.\n",
    "\n",
    "* **Your model should achieve a minimum BLEU score of 90 on the training set.**\n",
    "* The BLEU score on the test set should be greater than 40.\n",
    "\n",
    "The model can severly overfit to the training set and we do not cope with the overfitting problem in this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "51d6a35b286e16218271c80309237a07",
     "grade": false,
     "grade_id": "cell-c7c83e4783126f91",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from torchtext.data.metrics import bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b14071c9e20b3ed2969814459ebf12f9",
     "grade": true,
     "grade_id": "cell-d9e870bdbb7a77aa",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score on training data: 96.67546436378815\n"
     ]
    }
   ],
   "source": [
    "# Create translations for the training set\n",
    "candidate_corpus = []\n",
    "references_corpus = []\n",
    "for pad_src_seqs, src_seq_lengths, pad_tgt_seqs in trainloader:\n",
    "    out_seqs = translate(encoder, decoder, pad_src_seqs, src_seq_lengths)\n",
    "    candidate_corpus.extend([seq_to_tokens(seq, trainset.output_lang) for seq in out_seqs.T])\n",
    "    references_corpus.extend([[seq_to_tokens(seq, trainset.output_lang)] for seq in pad_tgt_seqs.T])\n",
    "\n",
    "# Compute BLEU for translations\n",
    "score = bleu_score(candidate_corpus, references_corpus)\n",
    "print(f'BLEU score on training data: {score*100}')\n",
    "assert score*100 > 90, \"The BLEU score is too low.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "156c000eef2a4d8dfcb638e3d77403d8",
     "grade": true,
     "grade_id": "cell-6960c91a78696fe7",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score on test data: 47.934204284767\n"
     ]
    }
   ],
   "source": [
    "# Create translations for the test set\n",
    "candidate_corpus = []\n",
    "references_corpus = []\n",
    "for pad_src_seqs, src_seq_lengths, pad_tgt_seqs in testloader:\n",
    "    out_seqs = translate(encoder, decoder, pad_src_seqs, src_seq_lengths)\n",
    "    candidate_corpus.extend([seq_to_tokens(seq, testset.output_lang) for seq in out_seqs.T])\n",
    "    references_corpus.extend([[seq_to_tokens(seq, testset.output_lang)] for seq in pad_tgt_seqs.T])\n",
    "\n",
    "# Compute BLEU for translations\n",
    "score = bleu_score(candidate_corpus, references_corpus)\n",
    "print(f'BLEU score on test data: {score*100}')\n",
    "assert score*100 > 40, \"The BLEU score is too low.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7bb3216ab2284b419d084dd20e6b6283",
     "grade": false,
     "grade_id": "cell-3e93ef4953126093",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Conclusion</b>\n",
    "</div>\n",
    "\n",
    "In this notebook:\n",
    "* We learned how recurrent neural networks can be used to build a sequence-to-sequence model.\n",
    "* We trained a sequence-to-sequence model for statistical machine translation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
